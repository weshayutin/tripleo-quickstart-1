# This directory is used to store a variety of files generated
# during the deploy process (ansible inventory, ssh configuration, ssh
# key files, etc)
local_working_dir: "{{ lookup('env', 'HOME') }}/.quickstart"

# These defaults are used if there are no flavor-specific
# overrides configured.
default_disk: 50
default_memory: 8192
default_vcpu: 1

# The undercloud needs more than the default amount of memory
# and disk.
undercloud_memory: 12288
undercloud_disk: 50
undercloud_vcpu: 4

# The default deployment has flavors for compute, controllers, ceph
# nodes, and undercloud nodes.  All flavors defined in the `flavors`
# key will be created with an `oooq_` prefix to avoid conflicts with
# the pre-defined flavors created by `openstack install undercloud`.
flavors:
  compute:
    memory: '{{compute_memory|default(default_memory)}}'
    disk: '{{compute_disk|default(default_disk)}}'
    vcpu: '{{compute_vcpu|default(default_vcpu)}}'

  control:
    memory: '{{control_memory|default(default_memory)}}'
    disk: '{{control_disk|default(default_disk)}}'
    vcpu: '{{control_vcpu|default(default_vcpu)}}'

  ceph:
    memory: '{{ceph_memory|default(default_memory)}}'
    disk: '{{ceph_disk|default(default_disk)}}'
    vcpu: '{{ceph_vcpu|default(default_vcpu)}}'

  undercloud:
    memory: '{{undercloud_memory|default(default_memory)}}'
    disk: '{{undercloud_disk|default(default_disk)}}'
    vcpu: '{{undercloud_vcpu|default(default_vcpu)}}'

# We create a single undercloud node.
undercloud_node:
  name: undercloud
  flavor: undercloud

# The overcloud will have three controllers, one compute node,
# and a ceph storage node.
overcloud_nodes:
  - name: control_0
    flavor: control
  - name: control_1
    flavor: control
  - name: control_2
    flavor: control

  - name: compute_0
    flavor: compute

  - name: ceph_0
    flavor: ceph

# Describe our virtual networks.  These networks will be attached to
# the undercloud node and to the overcloud nodes (except for the
# "external" network) in the order in which they are defined.  The
# playbooks expect to find both an "external" network and a
# "overcloud" network.
external_network_cidr: 192.168.23.1/24
networks:
  - name: external
    bridge: brext
    forward_mode: nat
    address: "{{ external_network_cidr|ipaddr('address') }}"
    netmask: "{{ external_network_cidr|ipaddr('netmask') }}"
    dhcp_range:
      - "{{ external_network_cidr|nthhost(10) }}"
      - "{{ external_network_cidr|nthhost(50) }}"
    nat_port_range:
      - 1024
      - 65535

  - name: overcloud
    bridge:  brovc

#Enable network isolation with single-nic-vlans for virtualized deployments
undercloud_external_network_cidr: 10.0.0.1/24
undercloud_networks:
  external:
    address: "{{ undercloud_external_network_cidr|ipaddr('address') }}"
    netmask: "{{ undercloud_external_network_cidr|ipaddr('netmask') }}"
    device_type: ovs
    type: OVSIntPort
    ovs_bridge: br-ctlplane
    ovs_options: '"tag=10"'
    tag: 10

network_isolation: true
ipv6: false

# Set this to `true` if you want your undercloud and overcloud vms to
# have a VNC console available.
enable_vnc_console: false

# We have some version specific behaviors, so we need a release variable
#
# TODO(trown): It would be better to write a release into the image itself
# and set this variable from there.
release: mitaka
